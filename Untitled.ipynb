{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199700cd",
   "metadata": {},
   "source": [
    "## * 参考给定LSTM+GRU的网址，对基本文本数据先进行处理。\n",
    "## *  对数据进行分别进行词袋技术、TF—IDF、词向量(torch.nn的，参考LSTM+GRU的网址)技术处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9835b2",
   "metadata": {},
   "source": [
    "## 1 predeal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1392279b",
   "metadata": {},
   "source": [
    "### 1 get env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd88b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./amazon_data/input\\test.ft.txt.bz2\n",
      "./amazon_data/input\\train.ft.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./amazon_data/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee79dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d702408",
   "metadata": {},
   "source": [
    "### 2 get input  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "959a0feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('./amazon_data/input/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('./amazon_data/input/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ac86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871f646",
   "metadata": {},
   "source": [
    "### 3 select part of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "984eb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pct of the data used for the models\n",
    "pct = 0.3\n",
    "num_train = int(len(train_file)*pct) #max 3600000\n",
    "num_test = int(len(test_file)*pct) #max 400000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f01a9",
   "metadata": {},
   "source": [
    "### 4 convert byte to utf-8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c8351a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_train]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55adc06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"__label__2 The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\\n\",\n",
       " '__label__2 Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you\\'ve played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time\\'s Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer\\'s work (I haven\\'t heard the Xenogears soundtrack, so I can\\'t say for sure), and even if you\\'ve never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\\n',\n",
       " \"__label__2 Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\\n\",\n",
       " \"__label__2 Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\\n\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29414724",
   "metadata": {},
   "source": [
    "### 5 extract labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cedaac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de16cf",
   "metadata": {},
   "source": [
    "### 6 data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c0a89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd0bca",
   "metadata": {},
   "source": [
    "### 7 data formalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75e71d",
   "metadata": {},
   "source": [
    "#### 1 URLs to \\<url>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78dcd17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1cb98",
   "metadata": {},
   "source": [
    "#### 2 tokenization of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba772ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "1.8518518518518519% done\n",
      "3.7037037037037037% done\n",
      "5.555555555555555% done\n",
      "7.407407407407407% done\n",
      "9.25925925925926% done\n",
      "11.11111111111111% done\n",
      "12.962962962962964% done\n",
      "14.814814814814815% done\n",
      "16.666666666666668% done\n",
      "18.51851851851852% done\n",
      "20.37037037037037% done\n",
      "22.22222222222222% done\n",
      "24.074074074074073% done\n",
      "25.925925925925927% done\n",
      "27.77777777777778% done\n",
      "29.62962962962963% done\n",
      "31.48148148148148% done\n",
      "33.333333333333336% done\n",
      "35.18518518518518% done\n",
      "37.03703703703704% done\n",
      "38.888888888888886% done\n",
      "40.74074074074074% done\n",
      "42.592592592592595% done\n",
      "44.44444444444444% done\n",
      "46.2962962962963% done\n",
      "48.148148148148145% done\n",
      "50.0% done\n",
      "51.851851851851855% done\n",
      "53.7037037037037% done\n",
      "55.55555555555556% done\n",
      "57.407407407407405% done\n",
      "59.25925925925926% done\n",
      "61.111111111111114% done\n",
      "62.96296296296296% done\n",
      "64.81481481481481% done\n",
      "66.66666666666667% done\n",
      "68.51851851851852% done\n",
      "70.37037037037037% done\n",
      "72.22222222222223% done\n",
      "74.07407407407408% done\n",
      "75.92592592592592% done\n",
      "77.77777777777777% done\n",
      "79.62962962962963% done\n",
      "81.48148148148148% done\n",
      "83.33333333333333% done\n",
      "85.18518518518519% done\n",
      "87.03703703703704% done\n",
      "88.88888888888889% done\n",
      "90.74074074074075% done\n",
      "92.5925925925926% done\n",
      "94.44444444444444% done\n",
      "96.29629629629629% done\n",
      "98.14814814814815% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    # The sentences will be stored as a list of words/tokens\n",
    "    train_sentences[i] = []\n",
    "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
    "        words.update([word.lower()])  # Converting all the words to lowercase\n",
    "        train_sentences[i].append(word)\n",
    "    if i%20000 == 0:\n",
    "        print(str((i*100)/num_train) + \"% done\")\n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1851e1a",
   "metadata": {},
   "source": [
    "#### 3 remove words only appeared once & add new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a624c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove appeared-once words\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "# sort words' appeared-num\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# add new words & padding \n",
    "words = ['_PAD','_UNK'] + words\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39339b24",
   "metadata": {},
   "source": [
    "### 8 translate words to corresponding indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "602dcec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(train_sentences):\n",
    "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    # do the same thing to test sentences\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46baff2",
   "metadata": {},
   "source": [
    "### 9 padding the sentences with 0s & shorten sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a46912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "test_sentences = pad_input(test_sentences, seq_len)\n",
    "\n",
    "# Converting our labels into numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e130b9f",
   "metadata": {},
   "source": [
    "### 10 split train/test/val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee79f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.5 # 50% validation, 50% test\n",
    "split_id = int(split_frac * len(test_sentences))\n",
    "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee040f",
   "metadata": {},
   "source": [
    "### 11 create model in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65ad7123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 400\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1bac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
